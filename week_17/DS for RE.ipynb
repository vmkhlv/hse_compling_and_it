{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-Su8ivYhz8R"
   },
   "source": [
    "# Relation extraction using distant supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKX5uPT_kuGH"
   },
   "source": [
    "Семинар подготовлен на основе Стенфордского курса CS224U, ссылка на их [материалы](http://web.stanford.edu/class/cs224u/2019/).\n",
    "\n",
    "Статьи про Distant supervision:\n",
    "Mintz et al. 2009  [Distant supervision for relation extraction without labeled data](https://www.aclweb.org/anthology/P09-1113/)\n",
    "\n",
    "\n",
    "\n",
    "Ye, Z.-X., and Ling, Z.-H. 2019 [Distant Supervision Relation Extraction with Intra-Bag\n",
    "and Inter-Bag Attentions\n",
    "](https://arxiv.org/pdf/1904.00143.pdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVY1Z_0Khz8T"
   },
   "source": [
    "# Relation Extraction задача в общем виде\n",
    "\n",
    "```\n",
    "(founders, SpaceX, Elon_Musk)\n",
    "(has_spouse, Elon_Musk, Talulah_Riley)\n",
    "(worked_at, Elon_Musk, Tesla_Motors)\n",
    "```\n",
    "Приложение - создание и пополнение баз знаний (knowledge base NB). Такая база полезна для множества задач, например для вопросно-ответных систем.\n",
    "\n",
    "\n",
    "### Distant supervision\n",
    "Мы хотим использовать обучение с учителем, но не хотим тратить ресурсы на ручную разметку. distant supervision способ использовать уже существующие в структурированном виде знания из базы знаний для получения новых. Зная отношение `(founders, SpaceX, Elon_Musk)` , мы делаем предположение, что предложения, где есть те же участники, выражают то же отношение. Разметим такие предложения автоматически и добавим их в наше обучающее множество. \n",
    "\n",
    "Проблемы подхода:\n",
    "- мы получаем зашумленные данные\n",
    "- нам нужно с чего-то начинать, мы не можем создавать KB с нуля и не можем находить новые отношения при таком подходе.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 2091,
     "status": "ok",
     "timestamp": 1615563094359,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "8XdilE8yhz8T"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZsT98pBhz8U"
   },
   "source": [
    "## The corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEUyECtzhz8U"
   },
   "source": [
    "Нам нужен корпус с отмеченными в нем сущностями.\n",
    "Должны соблюдаться два условия:\n",
    "1. Снята омонимия (sense disambiguation)\n",
    "1. Если одна и та же сущность может быть выражена по-разному, все упоминания сущности в корпусе должны указывать на один идентификатор (entity resolution)\n",
    "\n",
    "Мы будем использовать корпус на основе [Wikilinks](https://code.google.com/archive/p/wiki-links/). Google анонсировал Wikilinks в [2013](https://research.googleblog.com/2013/03/learning-from-big-data-40-million.html). В корпусе 40 миллионов упоминаний, 3 миллиона NE, каждое упоминание размечено ссылкой на  Википедию.\n",
    "\n",
    "Сегодняшний корпус - преобразованная версия Wikilinks. Каждый пример в нем имеет два упоминания сущностей и окружающий их контекст.\n",
    "Для удобства обращения с примерами, будем использовать класс `Corpus`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1408,
     "status": "ok",
     "timestamp": 1615563244156,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "jDH6pfsncPIl"
   },
   "outputs": [],
   "source": [
    "Example = namedtuple('Example',\n",
    "    'entity_1, entity_2, left, mention_1, middle, mention_2, right, '\n",
    "    'left_POS, mention_1_POS, middle_POS, mention_2_POS, right_POS')\n",
    "\n",
    "class Corpus(object):\n",
    "    \"\"\"\n",
    "    Class for representing and working with the raw text we use\n",
    "    as evidence for making relation predictions.\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_filename_or_examples : str or list\n",
    "        If str, this is assumed to be the full path to the gzip file\n",
    "        that contains the examples to use. The method `read_examples`\n",
    "        is used to open it in that case. If this is a list, then it\n",
    "        should be a list of `Example` instances.\n",
    "    Attributes\n",
    "    ----------\n",
    "    examples_by_entities : dict\n",
    "        A 2d dictionary mapping `ex.entity_1` to a dict mapping entity\n",
    "        `ex.entity_2` to the full `Example` instance `ex`. This is\n",
    "        created by the method `_index_examples_by_entities`.\n",
    "    \"\"\"\n",
    "    def __init__(self, src_filename_or_examples):\n",
    "        if isinstance(src_filename_or_examples, str):\n",
    "            self.examples = self.read_examples(src_filename_or_examples)\n",
    "        else:\n",
    "            self.examples = src_filename_or_examples\n",
    "        self.examples_by_entities = {}\n",
    "        self._index_examples_by_entities()\n",
    "\n",
    "    @staticmethod\n",
    "    def read_examples(src_filename):\n",
    "        \"\"\"\n",
    "        Read `src_filename`, assumed to be a `gzip` file with\n",
    "        tab-separated lines that can be turned into `Example`\n",
    "        instances.\n",
    "         Parameters\n",
    "        ----------\n",
    "        src_filename :  str\n",
    "            Assumed to be the full path to the gzip file that contains\n",
    "            the examples.\n",
    "        Returns\n",
    "        -------\n",
    "        list of Example\n",
    "        \"\"\"\n",
    "        examples = []\n",
    "        with gzip.open(src_filename, mode='rt', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                fields = line[:-1].split('\\t')\n",
    "                examples.append(Example(*fields))\n",
    "        return examples\n",
    "\n",
    "    def _index_examples_by_entities(self):\n",
    "        \"\"\"\n",
    "        Fill `examples_by_entities` as a 2d dictionary mapping\n",
    "        `ex.entity_1` to a dict mapping entity `ex.entity_2` to the\n",
    "        full `Example` instance `ex`.\n",
    "        \"\"\"\n",
    "        for ex in self.examples:\n",
    "            if ex.entity_1 not in self.examples_by_entities:\n",
    "                self.examples_by_entities[ex.entity_1] = {}\n",
    "            if ex.entity_2 not in self.examples_by_entities[ex.entity_1]:\n",
    "                self.examples_by_entities[ex.entity_1][ex.entity_2] = []\n",
    "            self.examples_by_entities[ex.entity_1][ex.entity_2].append(ex)\n",
    "\n",
    "    def get_examples_for_entities(self, e1, e2):\n",
    "        \"\"\"\n",
    "        Given two entities `e1` and `e2` as strings, return\n",
    "        examples from `self.examples_by_entities`, as a list of\n",
    "        `Example` instances.\"\"\"\n",
    "        try:\n",
    "            return self.examples_by_entities[e1][e2]\n",
    "        except KeyError:\n",
    "            return []\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Corpus with {0:,} examples'.format(len(self.examples))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8414,
     "status": "ok",
     "timestamp": 1615563308604,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "U8eBS5abhz8U",
    "outputId": "284dc09d-f75e-4bdf-ffe4-d240edc4201b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331696"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = Corpus('corpus.tsv.gz')\n",
    "\n",
    "len(corpus.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1121,
     "status": "ok",
     "timestamp": 1615563323923,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "7PipHbO5hz8W",
    "outputId": "ba3d29a7-e077-49c5-e07d-943087693d3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example(entity_1='The_Official_Story', entity_2='The_Secret_in_Their_Eyes', left='to well connected political families . These and other topics related to the Dirty War have been explored in books and films including Apartment Zero ,', mention_1='The Official Story', middle='and', mention_2='The Secret In Their Eyes .', right='Holtzman , who lives in Philadelphia , was born in Buenos Aires in 1947 and grew up in Latin America and Southeast Asia . He moved to the', left_POS='to/TO well/RB connected/JJ political/JJ families/NNS ./. These/DT and/CC other/JJ topics/NNS related/VBN to/TO the/DT Dirty/NNP War/NNP have/VBP been/VBN explored/VBN in/IN books/NNS and/CC films/NNS including/VBG Apartment/NNP Zero/NNP ,/,', mention_1_POS='The/DT Official/NNP Story/NNP', middle_POS='and/CC', mention_2_POS='The/DT Secret/JJ In/IN Their/PRP$ Eyes/NNS ./.', right_POS='Holtzman/NNP ,/, who/WP lives/VBZ in/IN Philadelphia/NNP ,/, was/VBD born/VBN in/IN Buenos/NNP Aires/NNP in/IN 1947/CD and/CC grew/VBD up/RP in/IN Latin/NNP America/NNP and/CC Southeast/NNP Asia/NNP ./. He/PRP moved/VBD to/TO the/DT')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.examples[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtnLMy12hz8W"
   },
   "source": [
    "В полях entity_1 entity_2  лежат названия википедийных статей, соотвествующих сущности. https://en.wikipedia.org/wiki/The_Secret_in_Their_Eyes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfyqplh-Oysz"
   },
   "outputs": [],
   "source": [
    "ex = corpus.examples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 903,
     "status": "ok",
     "timestamp": 1615476934505,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "sd2ybpYo90Dz",
    "outputId": "efb6ad5a-6139-4281-dedd-a2d291742f82"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'to all Spanish-occupied lands . The horno has a beehive shape and uses wood as the only heat source . The procedure still used in parts ofNew MexicoandArizonais to build a fire inside the Horno and , when the proper amount of time has passed , remove the embers and ashes and insert the'"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.left + ex.mention_1 + ex.middle + ex.mention_2 + ex.right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TnF-Ctjhz8X"
   },
   "source": [
    "Посмотрим на самые распространенные NE в корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1435,
     "status": "ok",
     "timestamp": 1615563410271,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "ooZf9vvshz8X",
    "outputId": "0072c37e-13df-4c91-8eb3-64dd2f2b9e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 95909 entities\n",
      "The most common entities are:\n",
      "      8137 India\n",
      "      5240 England\n",
      "      4121 France\n",
      "      4040 Germany\n",
      "      3937 Australia\n",
      "      3779 Canada\n",
      "      3633 Italy\n",
      "      3138 California\n",
      "      2894 New_York_City\n",
      "      2745 Pakistan\n",
      "      2213 New_Zealand\n",
      "      2183 New_York\n",
      "      2148 United_Kingdom\n",
      "      2030 Spain\n",
      "      2005 Japan\n",
      "      1891 Russia\n",
      "      1806 Philippines\n",
      "      1748 Malaysia\n",
      "      1721 Indonesia\n",
      "      1670 China\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "for example in corpus.examples:\n",
    "    counter[example.entity_1] += 1\n",
    "    counter[example.entity_2] += 1\n",
    "print('The corpus contains {} entities'.format(len(counter)))\n",
    "counts = sorted([(count, key) for key, count in counter.items()], reverse=True)\n",
    "print('The most common entities are:')\n",
    "for count, key in counts[:20]:\n",
    "    print('{:10d} {}'.format(count, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V43Bi7kZhz8X"
   },
   "source": [
    "посмотрим на примеры для NE `Elon_Musk` и `Tesla_Motors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 975,
     "status": "ok",
     "timestamp": 1615563450944,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "35p4_KQmhz8X",
    "outputId": "caf1ac18-7fcb-432d-a91b-c52fd4ace45f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.get_examples_for_entities('Elon_Musk', 'Tesla_Motors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZMPNcLyhz8X"
   },
   "source": [
    "Порядок важен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1109,
     "status": "ok",
     "timestamp": 1615563478428,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "bvUdnb8zhz8X",
    "outputId": "bd937cf2-1862-4b12-855f-7b0e49035496"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.get_examples_for_entities('Tesla_Motors', 'Elon_Musk'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SleQlAlhz8Y"
   },
   "source": [
    "## База знаний (Knowledge base, KB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmLBs6ckhz8Y"
   },
   "source": [
    "Мы будем использовать подмножество базы знаний [Freebase](https://en.wikipedia.org/wiki/Freebase_(database)). Проект перестал существовать в 2016 году, но дампы можно найти в открытом доступе(freebase-easy.cs.uni-freiburg.de/dump/).\n",
    "Можно посмотреть, что есть во FreeBase и как ей можно пользоваться: http://freebase-easy.cs.uni-freiburg.de/browse/ \n",
    "\n",
    "\n",
    "База знаний может быть представлена как набор троек: _relation_,  _subject_, _object_. Пример таких троек:\n",
    "\n",
    "```\n",
    "(place_of_birth, Barack_Obama, Honolulu)\n",
    "(has_spouse, Barack_Obama, Michelle_Obama)\n",
    "(author, The_Audacity_of_Hope, Barack_Obama)\n",
    "```\n",
    "\n",
    "Давайте посмотрим на нашу базу, используем для этого класс `KB`, который проиндексирует ее как по NE, так и по отношениям.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1615563708940,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "GdCbc-TXgNHt"
   },
   "outputs": [],
   "source": [
    "KBTriple = namedtuple('KBTriple', 'rel, sbj, obj')\n",
    "\n",
    "class KB(object):\n",
    "    \"\"\"\n",
    "    Class for representing and working with the knowledge base.\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_filename_or_triples : str or list\n",
    "        If str, this is assumed to be the full path to the gzip file\n",
    "        that contains the KB. The method `read_kb_triples` is used to\n",
    "        open it in that case. If this is a list, then it should be a\n",
    "        list of `KBTriple` instances.\n",
    "    Attributes\n",
    "    ----------\n",
    "    all_relations : list\n",
    "        Built by `_index_kb_triples_by_relation` as a list of str.\n",
    "    all_entity_pairs : list\n",
    "        Built by `_collect_all_entity_pairs`, as a sorted list of\n",
    "        (subject, object) tuples.\n",
    "    kb_triples_by_relation : dict\n",
    "        Built by `_index_kb_triples_by_relation`, as a dict mapping\n",
    "        relations (str) to `KBTriple` lists.\n",
    "    kb_triples_by_entities : dict\n",
    "        Built by `_index_kb_triples_by_entities`, as a dict mapping\n",
    "        relations subject (str) to dict mapping object (str) to\n",
    "        `KBTriple` lists.\n",
    "    \"\"\"\n",
    "    def __init__(self, src_filename_or_triples):\n",
    "        if isinstance(src_filename_or_triples, str):\n",
    "            self.kb_triples = self.read_kb_triples(src_filename_or_triples)\n",
    "        else:\n",
    "            self.kb_triples = src_filename_or_triples\n",
    "        self.all_relations = []\n",
    "        self.all_entity_pairs = []\n",
    "        self.kb_triples_by_relation = {}\n",
    "        self.kb_triples_by_entities = {}\n",
    "        self._collect_all_entity_pairs()\n",
    "        self._index_kb_triples_by_relation()\n",
    "        self._index_kb_triples_by_entities()\n",
    "\n",
    "    @staticmethod\n",
    "    def read_kb_triples(src_filename):\n",
    "        \"\"\"\n",
    "        Read `src_filename`, assumed to be a `gzip` file with\n",
    "        tab-separated lines that can be turned into `KBTriple`\n",
    "        instances.\n",
    "        Parameters\n",
    "        ----------\n",
    "        src_filename :  str\n",
    "            Assumed to be the full path to the gzip file that contains\n",
    "            the triples\n",
    "        Returns\n",
    "        -------\n",
    "        list of KBTriple\n",
    "        \"\"\"\n",
    "        kb_triples = []\n",
    "        with gzip.open(src_filename, mode='rt', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                rel, sbj, obj = line[:-1].split('\\t')\n",
    "                kb_triples.append(KBTriple(rel, sbj, obj))\n",
    "        return kb_triples\n",
    "\n",
    "    def _collect_all_entity_pairs(self):\n",
    "        pairs = set()\n",
    "        for kbt in self.kb_triples:\n",
    "            pairs.add((kbt.sbj, kbt.obj))\n",
    "        self.all_entity_pairs = sorted(list(pairs))\n",
    "\n",
    "    def _index_kb_triples_by_relation(self):\n",
    "        for kbt in self.kb_triples:\n",
    "            if kbt.rel not in self.kb_triples_by_relation:\n",
    "                self.kb_triples_by_relation[kbt.rel] = []\n",
    "            self.kb_triples_by_relation[kbt.rel].append(kbt)\n",
    "        self.all_relations = sorted(list(self.kb_triples_by_relation))\n",
    "\n",
    "    def _index_kb_triples_by_entities(self):\n",
    "        for kbt in self.kb_triples:\n",
    "            if kbt.sbj not in self.kb_triples_by_entities:\n",
    "                self.kb_triples_by_entities[kbt.sbj] = {}\n",
    "            if kbt.obj not in self.kb_triples_by_entities[kbt.sbj]:\n",
    "                self.kb_triples_by_entities[kbt.sbj][kbt.obj] = []\n",
    "            self.kb_triples_by_entities[kbt.sbj][kbt.obj].append(kbt)\n",
    "\n",
    "    def get_triples_for_relation(self, rel):\n",
    "        \"\"\"\"\n",
    "        Given a relation name (str), return all of the `KBTriple`\n",
    "        instances that involve it.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.kb_triples_by_relation[rel]\n",
    "        except KeyError:\n",
    "            return []\n",
    "\n",
    "    def get_triples_for_entities(self, e1, e2):\n",
    "        \"\"\"\n",
    "        Given a pair of entities `e1` and `e2` (both str), return\n",
    "        all of the `KBTriple` instances that involve them.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.kb_triples_by_entities[e1][e2]\n",
    "        except KeyError:\n",
    "            return []\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'KB with {0:,} triples'.format(len(self.kb_triples))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.kb_triples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2102,
     "status": "ok",
     "timestamp": 1615563729984,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "ypTczSqphz8Y",
    "outputId": "c5a018c9-6ffe-4ea0-a244-297f55d6b7c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45884"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb = KB('kb.tsv.gz')\n",
    "\n",
    "len(kb.kb_triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_YyEFBYhz8Y"
   },
   "source": [
    "\n",
    "Посмотрим \n",
    "- Сколько всего типов отношений\n",
    "- Количество примеров на отношение\n",
    "- Примеры троек\n",
    "- Сколько уникальных NE упоминается в базе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1037,
     "status": "ok",
     "timestamp": 1615563769562,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "dkMzwtgqhz8Y",
    "outputId": "61f88a4a-277f-4d2a-cbb5-de80cd0f0c40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kb.all_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1272,
     "status": "ok",
     "timestamp": 1615563831489,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "0VHc6zKqhz8Y",
    "outputId": "4f32c3d4-39df-4ef0-d00c-e1e268c456e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1702 adjoins\n",
      "2671 author\n",
      "522 capital\n",
      "18681 contains\n",
      "3947 film_performance\n",
      "1960 founders\n",
      "824 genre\n",
      "2563 has_sibling\n",
      "2994 has_spouse\n",
      "2542 is_a\n",
      "1598 nationality\n",
      "1586 parents\n",
      "1097 place_of_birth\n",
      "831 place_of_death\n",
      "1216 profession\n",
      "1150 worked_at\n"
     ]
    }
   ],
   "source": [
    "for rel in kb.all_relations:\n",
    "    print('{} {}'.format(len(kb.get_triples_for_relation(rel)), rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 993,
     "status": "ok",
     "timestamp": 1615563839654,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "VwflVF9Rhz8Y",
    "outputId": "f6018ebe-4ca2-4f5f-df47-f13e9264ca36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('adjoins', 'France', 'Spain')\n",
      "('author', 'Uncle_Silas', 'Sheridan_Le_Fanu')\n",
      "('capital', 'Panama', 'Panama_City')\n",
      "('contains', 'Brickfields', 'Kuala_Lumpur_Sentral_railway_station')\n",
      "('film_performance', 'Colin_Hanks', 'The_Great_Buck_Howard')\n",
      "('founders', 'Lashkar-e-Taiba', 'Hafiz_Muhammad_Saeed')\n",
      "('genre', '8_Simple_Rules', 'Sitcom')\n",
      "('has_sibling', 'Ari_Emanuel', 'Rahm_Emanuel')\n",
      "('has_spouse', 'Percy_Bysshe_Shelley', 'Mary_Shelley')\n",
      "('is_a', 'Bhanu_Athaiya', 'Costume_designer')\n",
      "('nationality', 'Ruben_Rausing', 'Sweden')\n",
      "('parents', 'Rosanna_Davison', 'Chris_de_Burgh')\n",
      "('place_of_birth', 'William_Penny_Brookes', 'Much_Wenlock')\n",
      "('place_of_death', 'Jean_Drapeau', 'Montreal')\n",
      "('profession', 'Rufus_Wainwright', 'Actor')\n",
      "('worked_at', 'Brian_Greene', 'Columbia_University')\n"
     ]
    }
   ],
   "source": [
    "for rel in kb.all_relations:\n",
    "    print(tuple(kb.get_triples_for_relation(rel)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCKI6boehz8Z"
   },
   "source": [
    "С помощью метода `kb.get_triples_for_entities()` мы можем найти тройки по субъекту и объекту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 862,
     "status": "ok",
     "timestamp": 1615563857719,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "xWDft5R3hz8Z",
    "outputId": "0da531de-da55-4114-92e6-4103ad65bd58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KBTriple(rel='adjoins', sbj='France', obj='Germany')]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb.get_triples_for_entities('France', 'Germany')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgwOJt4qhz8Z"
   },
   "source": [
    "Некоторые отношения симметричны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1280,
     "status": "ok",
     "timestamp": 1615563871928,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "k8p4gFLYhz8Z",
    "outputId": "04681014-638d-4e48-cdf8-c38d4dff93fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KBTriple(rel='adjoins', sbj='Germany', obj='France')]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb.get_triples_for_entities('Germany', 'France')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9952,
     "status": "ok",
     "timestamp": 1607420472661,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "7aFP8vmCNVZq",
    "outputId": "24a740ce-7a7c-40ae-e0cc-979a81239330"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KBTriple(rel='founders', sbj='Tesla_Motors', obj='Elon_Musk')]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb.get_triples_for_entities('Tesla_Motors','Elon_Musk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9940,
     "status": "ok",
     "timestamp": 1607420472662,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "Ju8QFQTShz8Z",
    "outputId": "8d0dcf31-f9e3-4cfb-e8a7-19980214a9b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KBTriple(rel='worked_at', sbj='Elon_Musk', obj='Tesla_Motors')]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb.get_triples_for_entities('Elon_Musk', 'Tesla_Motors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1566,
     "status": "ok",
     "timestamp": 1615563894147,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "WhsH7jEshz8Z",
    "outputId": "e5f3784b-2d49-4853-e2b2-6a2dc8bb1148"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KBTriple(rel='has_sibling', sbj='Cleopatra', obj='Ptolemy_XIII_Theos_Philopator'),\n",
       " KBTriple(rel='has_spouse', sbj='Cleopatra', obj='Ptolemy_XIII_Theos_Philopator')]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb.get_triples_for_entities('Cleopatra', 'Ptolemy_XIII_Theos_Philopator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhDpvXaNhz8a"
   },
   "source": [
    "Сколько всего в базе уникальных сущностей и в скольких тройках они встречаются?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1051,
     "status": "ok",
     "timestamp": 1615563968521,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "J-TzGHzLhz8a",
    "outputId": "c4afa5d0-8451-4b05-96a5-30944f31f121"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 40141 entities\n",
      "The most common entities are:\n",
      "945 England\n",
      "786 India\n",
      "438 Italy\n",
      "414 France\n",
      "412 California\n",
      "400 Germany\n",
      "372 United_Kingdom\n",
      "366 Canada\n",
      "302 New_York_City\n",
      "247 New_York\n",
      "236 Australia\n",
      "219 Philippines\n",
      "215 Japan\n",
      "212 Scotland\n",
      "208 Russia\n",
      "198 Actor\n",
      "172 Pakistan\n",
      "170 Ontario\n",
      "169 Ireland\n",
      "168 New_Zealand\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "for rel in kb.kb_triples:\n",
    "    counter[rel.sbj] += 1\n",
    "    counter[rel.obj] += 1\n",
    "print('The corpus contains {} entities'.format(len(counter)))\n",
    "counts = sorted([(count, key) for key, count in counter.items()], reverse=True)\n",
    "print('The most common entities are:')\n",
    "for count, key in counts[:20]:\n",
    "    print('{} {}'.format(count, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBLvh7u6hz8a"
   },
   "source": [
    "В базе гораздо меньше сущностей, чем в корпусе. Смысл задачи в том, чтобы найти новые NE, связанные отношениями из заранее известного списка, и пополнить базу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWsm7ZDthz8a"
   },
   "source": [
    "## Постановка задачи\n",
    "\n",
    "когда мы говорим о RE, возможны следующие разновидности задачи\n",
    "\n",
    "- Что именно мы классифицируем\n",
    "    - Пару упоминаний сущностей (NE mentions) в контексте\n",
    "    - Пару сущностей\n",
    "- Что предсказываем\n",
    "    - 0 или одно конкретное отношение (многоклассовая классификация)\n",
    "    - n отношений для пары сущностей (multi-label классификация)\n",
    "\n",
    "\n",
    "В зависимости от подхода, мы по-разному будем связывать KB и корпус. \n",
    "- При __классификации упоминаний сущностей__ мы используем базу для разметки отдельных предложений.\n",
    "\n",
    "При __классификации самих сущностей__ мы используем все предложения из корпуса с этой парой для извлечения признаков, которые бы описывали саму пару. (Наш сегодняшний выбор)\n",
    "\n",
    "\n",
    "\n",
    "Итак, мы будем решать такую задачу: на вход система берет пару сущностей и на выход отдает отношение/я между ними. Сформированные тройки пополняют KB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 1112,
     "status": "ok",
     "timestamp": 1615564079337,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "q-JNWxgwiyXR"
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \"\"\"\n",
    "    Class for unifying a `Corpus` and a `KB`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : `Corpus`\n",
    "    kb : `KB`\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus, kb):\n",
    "        self.corpus = corpus\n",
    "        self.kb = kb\n",
    "\n",
    "    def find_unrelated_pairs(self):\n",
    "        unrelated_pairs = set()\n",
    "        for ex in self.corpus.examples:\n",
    "            if self.kb.get_triples_for_entities(ex.entity_1, ex.entity_2):\n",
    "                continue\n",
    "            if self.kb.get_triples_for_entities(ex.entity_2, ex.entity_1):\n",
    "                continue\n",
    "            unrelated_pairs.add((ex.entity_1, ex.entity_2))\n",
    "            unrelated_pairs.add((ex.entity_2, ex.entity_1))\n",
    "        return unrelated_pairs\n",
    "\n",
    "    def featurize(self, kbts_by_rel, featurizers, vectorizer=None, vectorize=True):\n",
    "        \"\"\"\n",
    "        Featurize by relation.\n",
    "        Parameters\n",
    "        ----------\n",
    "        kbts_by_rel : dict\n",
    "            A map from relation (str) to lists of `KBTriples`.\n",
    "        featurizers : list of func\n",
    "            Each function has to have the signature\n",
    "            `kbt, corpus, feature_counter`, where `kbt` is a `KBTriple`,\n",
    "            `corpus` is a `Corpus`, and `feature_counter` is a count\n",
    "            dictionary.\n",
    "        vectorizer : DictVectorizer or None:\n",
    "            If None, a new `DictVectorizer` is created and used via\n",
    "            `fit`. This is primarily for training. If not None, then\n",
    "            `transform` is used. This is primarily for testing.\n",
    "        vectorize: bool\n",
    "            If True, the feature functions in `featurizers` are presumed\n",
    "            to create feature dicts, and a `DictVectorizer` is used. If\n",
    "            False, then `featurizers` is required to have exactly one\n",
    "            function in it, and that function must return exactly the\n",
    "            sort of objects that the models in the model factory take\n",
    "            as inputs.\n",
    "        Returns\n",
    "        -------\n",
    "        feat_matrices_by_rel, vectorizer\n",
    "            where `feat_matrices_by_rel` is a dict mapping relation names\n",
    "            to (i) lists of representation if `vectorize=False`, else\n",
    "            to `np.array`s, and (ii) and `vectorizer` is a\n",
    "            `DictVectorizer` if `vectorize=True`, else None\n",
    "        \"\"\"\n",
    "        if not vectorize:\n",
    "\n",
    "            feat_matrices_by_rel = defaultdict(list)\n",
    "            if len(featurizers) != 1:\n",
    "                raise ValueError(\n",
    "                    \"If `vectorize=False`, the `featurizers` argument \"\n",
    "                    \"must contain exactly one function.\")\n",
    "            featurizer = featurizers[0]\n",
    "            for rel, kbts in kbts_by_rel.items():\n",
    "                for kbt in kbts:\n",
    "                    rep = featurizer(kbt, self.corpus)\n",
    "                    feat_matrices_by_rel[rel].append(rep)\n",
    "            return feat_matrices_by_rel, None\n",
    "\n",
    "        # Create feature counters for all instances (kbts).\n",
    "        feat_counters_by_rel = defaultdict(list)\n",
    "        for rel, kbts in kbts_by_rel.items():\n",
    "            for kbt in kbts:\n",
    "                feature_counter = Counter()\n",
    "                for featurizer in featurizers:\n",
    "                    feature_counter = featurizer(kbt, self.corpus, feature_counter)\n",
    "                feat_counters_by_rel[rel].append(feature_counter)\n",
    "        feat_matrices_by_rel = defaultdict(list)\n",
    "        # If we haven't been given a Vectorizer, create one and fit\n",
    "        # it to all the feature counters.\n",
    "        if vectorizer is None:\n",
    "            vectorizer = DictVectorizer(sparse=True)\n",
    "            def traverse_dicts():\n",
    "                for dict_list in feat_counters_by_rel.values():\n",
    "                    for d in dict_list:\n",
    "                        yield d\n",
    "            vectorizer.fit(traverse_dicts())\n",
    "        # Now use the Vectorizer to transform feature dictionaries\n",
    "        # into feature matrices.\n",
    "        for rel, feat_counters in feat_counters_by_rel.items():\n",
    "            feat_matrices_by_rel[rel] = vectorizer.transform(feat_counters)\n",
    "        return feat_matrices_by_rel, vectorizer\n",
    "\n",
    "    def build_dataset(self, include_positive=True, sampling_rate=0.1, seed=1):\n",
    "        unrelated_pairs = self.find_unrelated_pairs()\n",
    "        random.seed(seed)\n",
    "        unrelated_pairs = random.sample(\n",
    "            unrelated_pairs, int(sampling_rate * len(unrelated_pairs)))\n",
    "        kbts_by_rel = defaultdict(list)\n",
    "        labels_by_rel = defaultdict(list)\n",
    "        for index, rel in enumerate(self.kb.all_relations):\n",
    "            if include_positive:\n",
    "                for kbt in self.kb.get_triples_for_relation(rel):\n",
    "                    kbts_by_rel[rel].append(kbt)\n",
    "                    labels_by_rel[rel].append(True)\n",
    "            for sbj, obj in unrelated_pairs:\n",
    "                kbts_by_rel[rel].append(KBTriple(rel, sbj, obj))\n",
    "                labels_by_rel[rel].append(False)\n",
    "        return kbts_by_rel, labels_by_rel\n",
    "\n",
    "    def build_splits(self,\n",
    "            split_names=['tiny', 'train', 'dev'],\n",
    "            split_fracs=[0.01, 0.74, 0.25],\n",
    "            seed=1):\n",
    "        if len(split_names) != len(split_fracs):\n",
    "            raise ValueError('split_names and split_fracs must be of equal length')\n",
    "        if sum(split_fracs) != 1.0:\n",
    "            raise ValueError('split_fracs must sum to 1')\n",
    "        n = len(split_fracs) # for convenience only\n",
    "\n",
    "        def split_list(xs):\n",
    "            xs = sorted(xs) # sorted for reproducibility\n",
    "            if seed:\n",
    "                random.seed(seed)\n",
    "            random.shuffle(xs)\n",
    "            split_points = [0] + [int(round(frac * len(xs)))\n",
    "                                  for frac in np.cumsum(split_fracs)]\n",
    "            return [xs[split_points[i]:split_points[i + 1]] for i in range(n)]\n",
    "\n",
    "        # first, split the entities that appear as subjects in the KB\n",
    "        sbjs = list(set([kbt.sbj for kbt in self.kb.kb_triples]))\n",
    "        sbj_splits = split_list(sbjs)\n",
    "        sbj_split_dict = {sbj: i for i, split in enumerate(sbj_splits)\n",
    "                                 for sbj in split}\n",
    "        # next, split the KB triples based on their subjects\n",
    "        kbt_splits = [[kbt for kbt in self.kb.kb_triples if sbj_split_dict[kbt.sbj] == i]\n",
    "                      for i in range(n)]\n",
    "        # now split examples based on the entities they contain\n",
    "        ex_splits = [[] for i in range(n + 1)] # include an extra split\n",
    "        for ex in self.corpus.examples:\n",
    "            if ex.entity_1 in sbj_split_dict:\n",
    "                # if entity_1 is a sbj in the KB, assign example to split of that sbj\n",
    "                ex_splits[sbj_split_dict[ex.entity_1]].append(ex)\n",
    "            elif ex.entity_2 in sbj_split_dict:\n",
    "                # if entity_2 is a sbj in the KB, assign example to split of that sbj\n",
    "                ex_splits[sbj_split_dict[ex.entity_2]].append(ex)\n",
    "            else:\n",
    "                # otherwise, put in extra split to be redistributed\n",
    "                ex_splits[-1].append(ex)\n",
    "        # reallocate the examples that weren't assigned to a split on first pass\n",
    "        extra_ex_splits = split_list(ex_splits[-1])\n",
    "        ex_splits = [ex_splits[i] + extra_ex_splits[i] for i in range(n)]\n",
    "\n",
    "        # create a Corpus and a KB for each split\n",
    "        data = {}\n",
    "        for i in range(n):\n",
    "            data[split_names[i]] = Dataset(Corpus(ex_splits[i]), KB(kbt_splits[i]))\n",
    "        data['all'] = self\n",
    "        return data\n",
    "\n",
    "    def count_examples(self):\n",
    "        counter = Counter()\n",
    "        for rel in self.kb.all_relations:\n",
    "            for kbt in self.kb.get_triples_for_relation(rel):\n",
    "                # count examples in both forward and reverse directions\n",
    "                counter[rel] += len(self.corpus.get_examples_for_entities(kbt.sbj, kbt.obj))\n",
    "                counter[rel] += len(self.corpus.get_examples_for_entities(kbt.obj, kbt.sbj))\n",
    "        # report results\n",
    "        print('{:20s} {:>10s} {:>10s} {:>10s}'.format(\n",
    "            '', '', '', 'examples'))\n",
    "        print('{:20s} {:>10s} {:>10s} {:>10s}'.format(\n",
    "            'relation', 'examples', 'triples', '/triple'))\n",
    "        print('{:20s} {:>10s} {:>10s} {:>10s}'.format(\n",
    "            '--------', '--------', '-------', '-------'))\n",
    "        for rel in self.kb.all_relations:\n",
    "            nx = counter[rel]\n",
    "            nt = len(self.kb.get_triples_for_relation(rel))\n",
    "            print('{:20s} {:10d} {:10d} {:10.2f}'.format(\n",
    "                rel, nx, nt, 1.0 * nx / nt))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"{}; {}\".format(self.corpus, self.kb)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 1393,
     "status": "ok",
     "timestamp": 1615564085357,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "mWTAcE4Bhz8a"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(corpus, kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1Lq-q-Nhz8a"
   },
   "source": [
    "Для каждого из 16 отношений в базе, для каждой пары сущностей в отношении мы нашли в корпусе содержащие эту пару примеры. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1574,
     "status": "ok",
     "timestamp": 1615564089408,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "zFf8wcmQhz8a",
    "outputId": "422295ba-448b-441c-bfbf-33e5b2059015"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             examples\n",
      "relation               examples    triples    /triple\n",
      "--------               --------    -------    -------\n",
      "adjoins                   58854       1702      34.58\n",
      "author                    11768       2671       4.41\n",
      "capital                    7443        522      14.26\n",
      "contains                  75952      18681       4.07\n",
      "film_performance           8994       3947       2.28\n",
      "founders                   5846       1960       2.98\n",
      "genre                      1576        824       1.91\n",
      "has_sibling                8525       2563       3.33\n",
      "has_spouse                12013       2994       4.01\n",
      "is_a                       5112       2542       2.01\n",
      "nationality                3403       1598       2.13\n",
      "parents                    3802       1586       2.40\n",
      "place_of_birth             1657       1097       1.51\n",
      "place_of_death             1523        831       1.83\n",
      "profession                 1851       1216       1.52\n",
      "worked_at                  3226       1150       2.81\n"
     ]
    }
   ],
   "source": [
    "dataset.count_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMKp11Jihz8a"
   },
   "source": [
    "### Отрицательные примеры\n",
    "\n",
    "Мы получили большое количество примеров, иллюстрирующих определенные отношения. Чтобы тренировать классификатор, нам нужны и отрицательные примеры. В качестве таких примеров возьмем совместные упоминания таких NE, которые не связаны отношением в нашей базе знаний.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2184,
     "status": "ok",
     "timestamp": 1615564276562,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "R14A3KPahz8a",
    "outputId": "8c79daf3-2633-49d9-b618-9c14167c0409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 247405 unrelated pairs, including:\n",
      "    ('Le_Corbusier', 'Alvar_Aalto')\n",
      "    ('Philip_Glass', 'Robert_Frank')\n",
      "    ('Puerto_Rico', 'Boston')\n",
      "    ('Show_Boat', 'The_King_and_I')\n",
      "    ('Herat', 'Kandahar_Province')\n",
      "    ('Genre', 'Dance_music')\n",
      "    ('Iowa_Central_Community_College', 'Morrisville_State_College')\n",
      "    ('The_Fresh_Prince_of_Bel-Air', 'Baywatch')\n",
      "    ('Chris_Pratt', 'Casey_Bond')\n",
      "    ('Amit_Chaudhuri', 'Royal_Society_of_Literature')\n"
     ]
    }
   ],
   "source": [
    "unrelated_pairs = dataset.find_unrelated_pairs()\n",
    "print('Found {} unrelated pairs, including:'.format(len(unrelated_pairs)))\n",
    "for pair in list(unrelated_pairs)[:10]:\n",
    "    print('   ', pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhDDlu23hz8a"
   },
   "source": [
    "### Multi-label classification\n",
    "\n",
    "Почему мы выбираем multi-label классификацию? \n",
    "Реализуйте функцию, которая посчитает, сколько в базе есть случаев, когда между двумя NE существует несколько отношений.\n",
    "методы: kb.all_entity_pairs, kb.get_triples_for_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 1361,
     "status": "ok",
     "timestamp": 1615564361918,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "u45KJv81hz8b"
   },
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "for a, b in kb.all_entity_pairs:\n",
    "   rels = kb.get_triples_for_entities(a, b)\n",
    "   if len(rels) > 1:\n",
    "    rels = tuple([rel.rel for rel in rels])\n",
    "    counts[rels] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1615564367659,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "gHALLp99bXsr",
    "outputId": "6905a10f-5616-403e-e7b9-d9b682a82c3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('adjoins', 'contains'): 6,\n",
       "         ('author', 'founders'): 1,\n",
       "         ('capital', 'contains'): 207,\n",
       "         ('contains', 'adjoins'): 5,\n",
       "         ('contains', 'capital'): 196,\n",
       "         ('has_sibling', 'has_spouse'): 3,\n",
       "         ('has_spouse', 'has_sibling'): 4,\n",
       "         ('is_a', 'is_a'): 3,\n",
       "         ('is_a', 'is_a', 'is_a'): 1,\n",
       "         ('is_a', 'profession'): 615,\n",
       "         ('nationality', 'place_of_birth'): 32,\n",
       "         ('nationality', 'place_of_birth', 'place_of_death'): 1,\n",
       "         ('nationality', 'place_of_death'): 3,\n",
       "         ('nationality', 'worked_at'): 1,\n",
       "         ('parents', 'has_spouse'): 1,\n",
       "         ('place_of_birth', 'nationality'): 29,\n",
       "         ('place_of_birth', 'place_of_death'): 79,\n",
       "         ('place_of_death', 'nationality'): 6,\n",
       "         ('place_of_death', 'place_of_birth'): 64,\n",
       "         ('place_of_death', 'place_of_birth', 'nationality'): 2,\n",
       "         ('profession', 'is_a'): 601,\n",
       "         ('worked_at', 'parents'): 2})"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PqiAOHlhz8b"
   },
   "source": [
    "Большинство сочетаний отношений выглядит очень логичным. Поэтому мы будем решать именно multi-label, так как непонятно, как выбрать  одно отношение да пары сущностей из трех 'nationality', 'place_of_birth', 'place_of_death'.\n",
    "Самый простой способ решать multi-label классификацию - это  отдельно предсказывать бинарный выход для каждого отношения. Правда в таком  случае мы теряем возможность обнаружить связь между отношениями, которые, очевидно, связаны.\n",
    "То есть по сути это эквивалентно тому, чтобы бинарно классифицировать тройки relation-subject-object (существует или нет такое отношение между такими участниками). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZjxHadEhz8b"
   },
   "source": [
    "### Создание датасета для обучения\n",
    "\n",
    "  Для простоты можно тренировать отдельные классификаторы для каждой роли. Для обучение нам нужен датасет следующего вида: список троек `KBTriples` и список булевых значений по тройкам.\n",
    "  Положительные примеры мы берем из базы, отрицательные сеплируем из нерелевантных пар."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 3240,
     "status": "ok",
     "timestamp": 1615564562644,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "n4yUeo_Whz8b"
   },
   "outputs": [],
   "source": [
    "kbts_by_rel, labels_by_rel = dataset.build_dataset(\n",
    "    include_positive=True, sampling_rate=0.1, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 980,
     "status": "ok",
     "timestamp": 1615564570814,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "bYLXlL5Ahz8b",
    "outputId": "f3641e48-df14-41b8-c450-555bffd36386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KBTriple(rel='adjoins', sbj='Thailand', obj='Laos') True\n"
     ]
    }
   ],
   "source": [
    "print(kbts_by_rel['adjoins'][1], labels_by_rel['adjoins'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1615564572564,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "Qp3SvOd1hz8b",
    "outputId": "1b37d3b8-ec7e-45b6-c573-6938d641ea1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KBTriple(rel='capital', sbj='The_Bicentennial_Man', obj='Film') False\n"
     ]
    }
   ],
   "source": [
    "print(kbts_by_rel['capital'][637], labels_by_rel['capital'][637])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GL3QgrIvhz8b"
   },
   "source": [
    "### Разбиение данных\n",
    "\n",
    "Разобьем данные на train/ dev/ tiny (последнее - 1% от данных, с ним удобно проверять работоспособность моделей)\n",
    "\n",
    "разбиение нетривиальное, так как нам нужно разбить и корпус, и KB.\n",
    "При разбиении на сплиты стремимся к идеальной ситуации, когда сущности, встречающиеся в трейне, не встречаются в тесте, а обучающее подмножество KB содержало только сущности из трейна. Чтобы приблизиться к этому, разбиваем следующим образом:\n",
    "\n",
    "- разбиваем по сплитам NE из роли subject, так чтоб они не пересекались\n",
    "- разбиваем по сплитам тройки по их субъектам\n",
    "- разбиваем примеры из корпуса:\n",
    "\n",
    "  - если первая NE  из примера отнесена к сплиту, относим туда весь пример\n",
    "  - если вторая NE  из примера отнесена к сплиту, относим туда весь пример\n",
    "  - если ни одна сущность из примера не  относится к сплиту, приписываем пример сплиту рандомно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2595,
     "status": "ok",
     "timestamp": 1615564791515,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "F0RPQ0ulhz8b",
    "outputId": "38676098-05f9-4ff4-9a5a-6fd5e8d1bca0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': Corpus with 331,696 examples; KB with 45,884 triples,\n",
       " 'dev': Corpus with 79,219 examples; KB with 11,210 triples,\n",
       " 'tiny': Corpus with 3,474 examples; KB with 445 triples,\n",
       " 'train': Corpus with 249,003 examples; KB with 34,229 triples}"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = dataset.build_splits(\n",
    "    split_names=['tiny', 'train', 'dev'],\n",
    "    split_fracs=[0.01, 0.74, 0.25],\n",
    "    seed=1)\n",
    "\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-mvwDhihz8b"
   },
   "source": [
    "### Метрики\n",
    "\n",
    "Будем использовать макро-усреднение по f-мере. Так как в задаче наполнения KB нам не так страшно пропустить отношение, как добавить много ложных, будем использовать f0,5-меру (за низкую точность штрафует сильнее, чем за низкую полноту)\n",
    "\n",
    "Проводить эвалюацию будем ф-цией  `rel_ext.evaluate()`\n",
    "Передаем ей \n",
    "- `splits`: словарь инстансов класса `Dataset`\n",
    "- `classifier`: ф-ция, берущая на вход список троек `KBTriples` и возвращающая булев список предсказаний \n",
    "- `test_split`: на каком сплите тестировать\n",
    "- `verbose`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 927,
     "status": "ok",
     "timestamp": 1615565111443,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "XZNQ1QLtqA6t"
   },
   "outputs": [],
   "source": [
    "def print_statistics_header():\n",
    "    print('{:20s} {:>10s} {:>10s} {:>10s} {:>10s} {:>10s}'.format(\n",
    "        'relation', 'precision', 'recall', 'f-score', 'support', 'size'))\n",
    "    print('{:20s} {:>10s} {:>10s} {:>10s} {:>10s} {:>10s}'.format(\n",
    "        '-' * 18, '-' * 9, '-' * 9, '-' * 9, '-' * 9, '-' * 9))\n",
    "\n",
    "\n",
    "def print_statistics_row(rel, result):\n",
    "    print('{:20s} {:10.3f} {:10.3f} {:10.3f} {:10d} {:10d}'.format(rel, *result))\n",
    "\n",
    "\n",
    "def print_statistics_footer(avg_result):\n",
    "    print('{:20s} {:>10s} {:>10s} {:>10s} {:>10s} {:>10s}'.format(\n",
    "        '-' * 18, '-' * 9, '-' * 9, '-' * 9, '-' * 9, '-' * 9))\n",
    "    print('{:20s} {:10.3f} {:10.3f} {:10.3f} {:10d} {:10d}'.format('macro-average', *avg_result))\n",
    "\n",
    "\n",
    "def macro_average_results(results):\n",
    "    avg_result = [np.average([r[i] for r in results.values()]) for i in range(3)]\n",
    "    avg_result.append(np.sum([r[3] for r in results.values()]))\n",
    "    avg_result.append(np.sum([r[4] for r in results.values()]))\n",
    "    return avg_result\n",
    "\n",
    "\n",
    "def evaluate(splits, classifier, test_split='dev', sampling_rate=0.1, verbose=True):\n",
    "    test_kbts_by_rel, true_labels_by_rel = splits[test_split].build_dataset(sampling_rate=sampling_rate)\n",
    "    results = {}\n",
    "    if verbose:\n",
    "        print_statistics_header()\n",
    "    for rel in splits['all'].kb.all_relations:\n",
    "        pred_labels = classifier(test_kbts_by_rel[rel])\n",
    "        stats = precision_recall_fscore_support(true_labels_by_rel[rel], pred_labels, beta=0.5)\n",
    "        stats = [stat[1] for stat in stats]  # stats[1] is the stat for label True\n",
    "        stats.append(len(pred_labels)) # number of examples\n",
    "        results[rel] = stats\n",
    "        if verbose:\n",
    "            print_statistics_row(rel, results[rel])\n",
    "    avg_result = macro_average_results(results)\n",
    "    if verbose:\n",
    "        print_statistics_footer(avg_result)\n",
    "    return avg_result[2]  # return f_0.5 score as summary statistic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtL6ElZOhz8b"
   },
   "source": [
    "### Случайное угадывание\n",
    "\n",
    "Попробуем в качестве бейслайна, чтобы понять насколько все плохо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 2130,
     "status": "ok",
     "timestamp": 1615565170145,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "miwIpFcshz8c"
   },
   "outputs": [],
   "source": [
    "def lift(f):\n",
    "    return lambda xs: [f(x) for x in xs]\n",
    "\n",
    "def make_random_classifier(p=0.50):\n",
    "    def random_classify(kb_triple):\n",
    "        return random.random() < p\n",
    "    return lift(random_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1912,
     "status": "ok",
     "timestamp": 1615565171717,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "T6SgbKpohz8c",
    "outputId": "72822870-6de3-4059-fd40-369bf79f9610"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.062      0.543      0.075        407       7057\n",
      "author                    0.095      0.519      0.113        657       7307\n",
      "capital                   0.019      0.508      0.023        126       6776\n",
      "contains                  0.402      0.501      0.419       4487      11137\n",
      "film_performance          0.127      0.494      0.149        984       7634\n",
      "founders                  0.064      0.484      0.078        469       7119\n",
      "genre                     0.031      0.507      0.038        205       6855\n",
      "has_sibling               0.085      0.494      0.102        625       7275\n",
      "has_spouse                0.098      0.481      0.116        754       7404\n",
      "is_a                      0.085      0.503      0.102        618       7268\n",
      "nationality               0.062      0.567      0.076        386       7036\n",
      "parents                   0.055      0.513      0.068        390       7040\n",
      "place_of_birth            0.045      0.550      0.055        282       6932\n",
      "place_of_death            0.030      0.502      0.037        209       6859\n",
      "profession                0.044      0.500      0.054        308       6958\n",
      "worked_at                 0.041      0.472      0.050        303       6953\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.084      0.509      0.097      11210     117610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09720548338767715"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(splits, make_random_classifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leZ2EQM6hz8c"
   },
   "source": [
    "## Baseline  по частотным фразам между сущностями\n",
    "\n",
    "Идея для каждой роли посчитать частотные фразы между участниками. А классифицировать будем так: для новой пары сущностей если в каком-от из контекстов для этой пары встретились частотные фразы для определенного класса - относим ее в этот класс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1585,
     "status": "ok",
     "timestamp": 1615565450426,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "7zpNNJNAhz8c",
    "outputId": "f158cbaa-838c-4c5f-ffcc-6f39d01ca7a3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjoins              fwd           7667 ,\n",
      "adjoins              fwd           5134 and\n",
      "adjoins              fwd            903 , and\n",
      "adjoins              rev           4582 ,\n",
      "adjoins              rev           3000 and\n",
      "adjoins              rev            507 , and\n",
      "author               fwd           1007 by\n",
      "author               fwd            124 ,\n",
      "author               fwd            105 , by\n",
      "author               rev            816 's\n",
      "author               rev            210 ‘ s\n",
      "author               rev            142 ’ s\n",
      "capital              fwd             33 ,\n",
      "capital              fwd             17 , after\n",
      "capital              fwd             14 in\n",
      "capital              rev           2506 ,\n",
      "capital              rev            121 in\n",
      "capital              rev             73 , the capital of\n",
      "contains             fwd            319 's\n",
      "contains             fwd            296 ,\n",
      "contains             fwd            211 (\n",
      "contains             rev          18511 ,\n",
      "contains             rev           4160 in\n",
      "contains             rev            603 in the\n",
      "film_performance     fwd            283 in\n",
      "film_performance     fwd            151 's\n",
      "film_performance     fwd             96 film\n",
      "film_performance     rev            183 with\n",
      "film_performance     rev            128 , starring\n",
      "film_performance     rev             97 opposite\n",
      "founders             fwd             78 founder\n",
      "founders             fwd             56 co-founder\n",
      "founders             fwd             44 ,\n",
      "founders             rev            140 's\n",
      "founders             rev             66 ‘ s\n",
      "founders             rev             62 of the\n",
      "genre                fwd             20 , a\n",
      "genre                fwd             13 in 1994 , he became a central figure in the\n",
      "genre                fwd             11 is a\n",
      "genre                rev             98 ,\n",
      "genre                rev             60 series\n",
      "genre                rev             17 show\n",
      "has_sibling          fwd           1115 and\n",
      "has_sibling          fwd            545 ,\n",
      "has_sibling          fwd            125 , and\n",
      "has_sibling          rev            676 and\n",
      "has_sibling          rev            371 ,\n",
      "has_sibling          rev             68 , and\n",
      "has_spouse           fwd           1825 and\n",
      "has_spouse           fwd            379 ,\n",
      "has_spouse           fwd             97 and his wife\n",
      "has_spouse           rev           1183 and\n",
      "has_spouse           rev            225 ,\n",
      "has_spouse           rev             74 and his wife\n",
      "is_a                 fwd            100 ,\n",
      "is_a                 fwd             44 family ,\n",
      "is_a                 fwd             34 , a\n",
      "is_a                 rev            175 ,\n",
      "is_a                 rev             73 \n",
      "is_a                 rev             47 of\n",
      "nationality          fwd            264 of\n",
      "nationality          fwd             70 in\n",
      "nationality          fwd             27 from\n",
      "nationality          rev             51 ,\n",
      "nationality          rev             24 by\n",
      "nationality          rev             18 under\n",
      "parents              fwd             64 , son of\n",
      "parents              fwd             45 and\n",
      "parents              fwd             42 ,\n",
      "parents              rev            187 and\n",
      "parents              rev            151 ,\n",
      "parents              rev             42 and his son\n",
      "place_of_birth       fwd             85 of\n",
      "place_of_birth       fwd             50 was born in\n",
      "place_of_birth       fwd             35 in\n",
      "place_of_birth       rev             15 by\n",
      "place_of_birth       rev             15 ,\n",
      "place_of_birth       rev              9 -born Franciscan scholar\n",
      "place_of_death       fwd             65 in\n",
      "place_of_death       fwd             48 of\n",
      "place_of_death       fwd              9 at\n",
      "place_of_death       rev              9 ,\n",
      "place_of_death       rev              8 mayor\n",
      "place_of_death       rev              7 by\n",
      "profession           fwd             85 ,\n",
      "profession           fwd             27 , a\n",
      "profession           fwd             26 and\n",
      "profession           rev            101 ,\n",
      "profession           rev             67 \n",
      "profession           rev             24 and\n",
      "worked_at            fwd             94 of\n",
      "worked_at            fwd             57 at\n",
      "worked_at            fwd             57 's\n",
      "worked_at            rev             34 ,\n",
      "worked_at            rev             19 with\n",
      "worked_at            rev             18 co-founder\n"
     ]
    }
   ],
   "source": [
    "def find_common_middles(split, top_k=3, show_output=False):\n",
    "    corpus = split.corpus\n",
    "    kb = split.kb\n",
    "    mids_by_rel = {\n",
    "        'fwd': defaultdict(lambda: defaultdict(int)),\n",
    "        'rev': defaultdict(lambda: defaultdict(int))}\n",
    "    for rel in kb.all_relations:\n",
    "        for kbt in kb.get_triples_for_relation(rel):\n",
    "            for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "                mids_by_rel['fwd'][rel][ex.middle] += 1\n",
    "            for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "                mids_by_rel['rev'][rel][ex.middle] += 1\n",
    "    def most_frequent(mid_counter):\n",
    "        return sorted([(cnt, mid) for mid, cnt in mid_counter.items()], reverse=True)[:top_k]\n",
    "    for rel in kb.all_relations:\n",
    "        for dir in ['fwd', 'rev']:\n",
    "            top = most_frequent(mids_by_rel[dir][rel])\n",
    "            if show_output:\n",
    "                for cnt, mid in top:\n",
    "                    print('{:20} {:7} {:10} {:}'.format(rel, dir, cnt, mid))\n",
    "            mids_by_rel[dir][rel] = set([mid for cnt, mid in top])\n",
    "    return mids_by_rel\n",
    "\n",
    "_ = find_common_middles(splits['train'], show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 1991,
     "status": "ok",
     "timestamp": 1615565436791,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "ylIN0fwzhz8c"
   },
   "outputs": [],
   "source": [
    "def train_top_k_middles_classifier(top_k=3):\n",
    "    split = splits['train']\n",
    "    corpus = split.corpus\n",
    "    top_k_mids_by_rel = find_common_middles(split=split, top_k=top_k)\n",
    "    def classify(kb_triple):\n",
    "        fwd_mids = top_k_mids_by_rel['fwd'][kb_triple.rel]\n",
    "        rev_mids = top_k_mids_by_rel['rev'][kb_triple.rel]\n",
    "        for ex in corpus.get_examples_for_entities(kb_triple.sbj, kb_triple.obj):\n",
    "            if ex.middle in fwd_mids:\n",
    "                return True\n",
    "        for ex in corpus.get_examples_for_entities(kb_triple.obj, kb_triple.sbj):\n",
    "            if ex.middle in rev_mids:\n",
    "                return True\n",
    "        return False\n",
    "    return lift(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1721,
     "status": "ok",
     "timestamp": 1615565455653,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "O_EEHmuEhz8c",
    "outputId": "b0fd4c77-f50c-4c36-fa79-7b68c4614ee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.273      0.285      0.275        407       7057\n",
      "author                    0.323      0.078      0.198        657       7307\n",
      "capital                   0.089      0.159      0.098        126       6776\n",
      "contains                  0.582      0.064      0.222       4487      11137\n",
      "film_performance          0.312      0.005      0.024        984       7634\n",
      "founders                  0.150      0.038      0.095        469       7119\n",
      "genre                     0.000      0.000      0.000        205       6855\n",
      "has_sibling               0.263      0.176      0.239        625       7275\n",
      "has_spouse                0.349      0.211      0.309        754       7404\n",
      "is_a                      0.070      0.024      0.051        618       7268\n",
      "nationality               0.103      0.036      0.075        386       7036\n",
      "parents                   0.080      0.067      0.077        390       7040\n",
      "place_of_birth            0.016      0.007      0.013        282       6932\n",
      "place_of_death            0.024      0.014      0.021        209       6859\n",
      "profession                0.039      0.039      0.039        308       6958\n",
      "worked_at                 0.048      0.020      0.037        303       6953\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.170      0.076      0.111      11210     117610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11079552290042358"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(splits, train_top_k_middles_classifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9C0gOHa0IXhQ"
   },
   "source": [
    "### Baseline  на мешке слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4X7gPOi3IVtp"
   },
   "outputs": [],
   "source": [
    "def simple_bag_of_words_featurizer(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[word] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[word] += 1\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 1607,
     "status": "ok",
     "timestamp": 1615565557217,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "Ua9v3vRNppj5"
   },
   "outputs": [],
   "source": [
    "def train_models(\n",
    "        splits,\n",
    "        featurizers,\n",
    "        split_name='train',\n",
    "        model_factory=(lambda: LogisticRegression(\n",
    "            fit_intercept=True, solver='liblinear', random_state=42)),\n",
    "        sampling_rate=0.1,\n",
    "        vectorize=True,\n",
    "        verbose=True):\n",
    "    train_dataset = splits[split_name]\n",
    "    train_o, train_y = train_dataset.build_dataset(sampling_rate=sampling_rate)\n",
    "    train_X, vectorizer = train_dataset.featurize(\n",
    "        train_o, featurizers, vectorize=vectorize)\n",
    "    models = {}\n",
    "    for rel in splits['all'].kb.all_relations:\n",
    "        models[rel] = model_factory()\n",
    "        models[rel].fit(train_X[rel], train_y[rel])\n",
    "    return {\n",
    "        'featurizers': featurizers,\n",
    "        'vectorizer': vectorizer,\n",
    "        'models': models,\n",
    "        'all_relations': splits['all'].kb.all_relations,\n",
    "        'vectorize': vectorize}\n",
    "\n",
    "\n",
    "def predict(splits, train_result, split_name='dev', sampling_rate=0.1, vectorize=True):\n",
    "    assess_dataset = splits[split_name]\n",
    "    assess_o, assess_y = assess_dataset.build_dataset(sampling_rate=sampling_rate)\n",
    "    test_X, _ = assess_dataset.featurize(\n",
    "        assess_o,\n",
    "        featurizers=train_result['featurizers'],\n",
    "        vectorizer=train_result['vectorizer'],\n",
    "        vectorize=vectorize)\n",
    "    predictions = {}\n",
    "    for rel in train_result['all_relations']:\n",
    "        predictions[rel] = train_result['models'][rel].predict(test_X[rel])\n",
    "    return predictions, assess_y\n",
    "\n",
    "\n",
    "def evaluate_predictions(predictions, test_y, verbose=True):\n",
    "    results = {}  # one result row for each relation\n",
    "    if verbose:\n",
    "        print_statistics_header()\n",
    "    for rel, preds in predictions.items():\n",
    "        stats = precision_recall_fscore_support(test_y[rel], preds, beta=0.5)\n",
    "        stats = [stat[1] for stat in stats]  # stats[1] is the stat for label True\n",
    "        stats.append(len(test_y[rel]))\n",
    "        results[rel] = stats\n",
    "        if verbose:\n",
    "            print_statistics_row(rel, results[rel])\n",
    "    avg_result = macro_average_results(results)\n",
    "    if verbose:\n",
    "        print_statistics_footer(avg_result)\n",
    "    return avg_result[2]  # return f_0.5 score as summary statistic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1615565561451,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "ezUe7VunJEVh",
    "outputId": "9d8fa18b-9484-4d8d-e116-d4cddf3bf832"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KBTriple(rel='has_sibling', sbj='Ari_Emanuel', obj='Rahm_Emanuel')"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kbt = kb.kb_triples[2]\n",
    "kbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 718,
     "status": "ok",
     "timestamp": 1607427617705,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "VkbDuUvqJwj_",
    "outputId": "1767c089-9191-415b-b92a-650fc7b76c2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example(entity_1='Ari_Emanuel', entity_2='Rahm_Emanuel', left='stop copyright infringers , asking them to punish them as they would shoplifters . I guess Murdoch doesn ’ t understand the different between theft and copyright infringement . And', mention_1='Ari Emmanuel', middle=',', mention_2='Rahm Emmanuel', right='‘ s brother ( and the inspiration for the Entourage character Ari ) , has been lobbying President Obama to implement some sort of three-strikes policy , like they', left_POS=\"stop/VB copyright/NN infringers/NNS ,/, asking/VBG them/PRP to/TO punish/VB them/PRP as/IN they/PRP would/MD shoplifters/NNS ./. I/PRP guess/VBP Murdoch/NNP doesn/NN '/POS t/NN understand/VBP the/DT different/JJ between/IN theft/NN and/CC copyright/NN infringement/NN ./. And/CC\", mention_1_POS='Ari/NNP Emmanuel/NNP', middle_POS=',/,', mention_2_POS='Rahm/NNP Emmanuel/NNP', right_POS='`/`` s/NNS brother/NN -LRB-/-LRB- and/CC the/DT inspiration/NN for/IN the/DT Entourage/NN character/NN Ari/NNP -RRB-/-RRB- ,/, has/VBZ been/VBN lobbying/VBG President/NNP Obama/NNP to/TO implement/VB some/DT sort/NN of/IN three-strikes/JJ policy/NN ,/, like/IN they/PRP'),\n",
       " Example(entity_1='Ari_Emanuel', entity_2='Rahm_Emanuel', left='about this little La Mirage skeleton in his closet . Did he get on the phone to somebody who could fix it ? The White House Connection .', mention_1='Ari Emanuel', middle=\", brother of President Obama 's Chief of Staff ,\", mention_2='Rahm Emanuel', right=', is seen in Hollywood as a very powerful and aggressive player . He got into the talent agency business shortly after leaving college in 1983 .', left_POS='about/IN this/DT little/JJ La/NNP Mirage/NNP skeleton/NNP in/IN his/PRP$ closet/NN ./. Did/VBD he/PRP get/VB on/IN the/DT phone/NN to/TO somebody/NN who/WP could/MD fix/VB it/PRP ?/. The/DT White/NNP House/NNP Connection/NN ./.', mention_1_POS='Ari/NNP Emanuel/NNP', middle_POS=\",/, brother/NN of/IN President/NNP Obama/NNP 's/POS Chief/NNP of/IN Staff/NNP ,/,\", mention_2_POS='Rahm/NNP Emanuel/NNP', right_POS=',/, is/VBZ seen/VBN in/IN Hollywood/NNP as/IN a/DT very/RB powerful/JJ and/CC aggressive/JJ player/NN ./. He/PRP got/VBD into/IN the/DT talent/NN agency/NN business/NN shortly/RB after/IN leaving/VBG college/NN in/IN 1983/CD ./.'),\n",
       " Example(entity_1='Ari_Emanuel', entity_2='Rahm_Emanuel', left='It ’ s going to be a very thin , cheap device that can connect to the cloud . Or maybe you ’ ll see a hybrid solution in the meantime .', mention_1='Ari Emanuel', middle='- [ White House Chief of Staff ]', mention_2='Rahm Emanuel', right='’ s brother - is working in the White House , and his brother is in charge of how that stimulus money is spent . There have been', left_POS=\"It/PRP '/'' s/VBZ going/VBG to/TO be/VB a/DT very/RB thin/JJ ,/, cheap/JJ device/NN that/WDT can/MD connect/VB to/TO the/DT cloud/NN ./. Or/CC maybe/RB you/PRP '/'' ll/NN see/VB a/DT hybrid/NN solution/NN in/IN the/DT meantime/NN ./.\", mention_1_POS='Ari/NNP Emanuel/NNP', middle_POS='-/: -LSB-/-LRB- White/NNP House/NNP Chief/NNP of/IN Staff/NNP -RSB-/-RRB-', mention_2_POS='Rahm/NNP Emanuel/NNP', right_POS=\"'/POS s/NNS brother/NN -/: is/VBZ working/VBG in/IN the/DT White/NNP House/NNP ,/, and/CC his/PRP$ brother/NN is/VBZ in/IN charge/NN of/IN how/WRB that/DT stimulus/NN money/NN is/VBZ spent/VBN ./. There/EX have/VBP been/VBN\"),\n",
       " Example(entity_1='Ari_Emanuel', entity_2='Rahm_Emanuel', left=\"It 's going to be a very thin , cheap device that can connect to the cloud . Or maybe you 'll see a hybrid solution in the meantime .\", mention_1='Ari Emanuel', middle='-- [ White House Chief of Staff ]', mention_2='Rahm Emanuel', right=\"'s brother -- is working in the White House , and his brother is in charge of how that stimulus money is spent . There have been\", left_POS=\"It/PRP 's/VBZ going/VBG to/TO be/VB a/DT very/RB thin/JJ ,/, cheap/JJ device/NN that/WDT can/MD connect/VB to/TO the/DT cloud/NN ./. Or/CC maybe/RB you/PRP 'll/MD see/VB a/DT hybrid/NN solution/NN in/IN the/DT meantime/NN ./.\", mention_1_POS='Ari/NNP Emanuel/NNP', middle_POS='--/: -LSB-/-LRB- White/NNP House/NNP Chief/NNP of/IN Staff/NNP -RSB-/-RRB-', mention_2_POS='Rahm/NNP Emanuel/NNP', right_POS=\"'s/POS brother/NN --/: is/VBZ working/VBG in/IN the/DT White/NNP House/NNP ,/, and/CC his/PRP$ brother/NN is/VBZ in/IN charge/NN of/IN how/WRB that/DT stimulus/NN money/NN is/VBZ spent/VBN ./. There/EX have/VBP been/VBN\"),\n",
       " Example(entity_1='Ari_Emanuel', entity_2='Rahm_Emanuel', left='pool the cue sport , in which bank means both a cushion and to drive a ball into the cushion . 23d Ari { Talent agent ___ Emanuel } .', mention_1='Ari Emanuel', middle='is', mention_2='Rahm Emanuel', right=\"'s brother . Image of the Day 22a Paolo { Uccello who painted `` The Battle of San Romano '' } . The Battle of San Romano is a set of\", left_POS='pool/NN the/DT cue/NN sport/NN ,/, in/IN which/WDT bank/NN means/VBZ both/CC a/DT cushion/NN and/CC to/TO drive/VB a/DT ball/NN into/IN the/DT cushion/NN ./. 23d/JJ Ari/NNP -LCB-/-LRB- Talent/NNP agent/NN ___/CD Emanuel/NNP -RCB-/-RRB- ./.', mention_1_POS='Ari/NNP Emanuel/NNP', middle_POS='is/VBZ', mention_2_POS='Rahm/NNP Emanuel/NNP', right_POS=\"'s/POS brother/NN ./. Image/NN of/IN the/DT Day/NN 22a/NN Paolo/NNP -LCB-/-LRB- Uccello/NNP who/WP painted/VBD ``/`` The/DT Battle/NN of/IN San/NNP Romano/NNP ''/'' -RCB-/-RRB- ./. The/DT Battle/NN of/IN San/NNP Romano/NNP is/VBZ a/DT set/NN of/IN\"),\n",
       " Example(entity_1='Ari_Emanuel', entity_2='Rahm_Emanuel', left='2007 Sacha Baron Cohen ( aka Ali G , Borat , Bruno ) Oded Fehr ( Israeli/American actor ) Jeremy Piven ( his character on Entourage , Ari Gold , is based on', mention_1='Ari Emanuel', middle=', brother of congressman', mention_2='Rahm Emanuel', right=', on whom the West Wing character Josh Lyman is based . ) posted by junkbox at 8:04 PM on May 4 , 2007 Has no one said', left_POS='2007/CD Sacha/NNP Baron/NNP Cohen/NNP -LRB-/-LRB- aka/FW Ali/NNP G/NNP ,/, Borat/NNP ,/, Bruno/NNP -RRB-/-RRB- Oded/NNP Fehr/NNP -LRB-/-LRB- Israeli/American/JJ actor/NN -RRB-/-RRB- Jeremy/NNP Piven/NNP -LRB-/-LRB- his/PRP$ character/NN on/IN Entourage/NN ,/, Ari/NNP Gold/NNP ,/, is/VBZ based/VBN on/IN', mention_1_POS='Ari/NNP Emanuel/NNP', middle_POS=',/, brother/NN of/IN congressman/NN', mention_2_POS='Rahm/NNP Emanuel/NNP', right_POS=',/, on/IN whom/WP the/DT West/NNP Wing/NN character/NN Josh/NNP Lyman/NNP is/VBZ based/VBN ./. -RRB-/-RRB- posted/VBN by/IN junkbox/NN at/IN 8:04/CD PM/NN on/IN May/NNP 4/CD ,/, 2007/CD Has/VBZ no/DT one/NN said/VBD')]"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits['train'].corpus.get_examples_for_entities(kbt.sbj, kbt.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1607427633801,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "EWgZybOVKjED",
    "outputId": "0d270cdf-3bb8-464d-a9b3-4ff116bd59e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({\"'s\": 1,\n",
       "         ',': 4,\n",
       "         '-': 1,\n",
       "         '--': 1,\n",
       "         'Chief': 3,\n",
       "         'House': 2,\n",
       "         'Obama': 1,\n",
       "         'President': 1,\n",
       "         'Staff': 3,\n",
       "         'White': 2,\n",
       "         '[': 2,\n",
       "         ']': 2,\n",
       "         'brother': 2,\n",
       "         'congressman': 1,\n",
       "         'is': 1,\n",
       "         'of': 5})"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_bag_of_words_featurizer(kbt, splits['train'].corpus, Counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boaB0xLiL3WW"
   },
   "outputs": [],
   "source": [
    "featurized = dataset.featurize(kbts_by_rel, featurizers=[simple_bag_of_words_featurizer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 835,
     "status": "ok",
     "timestamp": 1607427709916,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "Y2iSEsn9PL1o",
    "outputId": "e4ef7791-006b-4838-ba60-a01cad9930e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25890"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kbts_by_rel['worked_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 623,
     "status": "ok",
     "timestamp": 1607427717677,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "5EN9F_ZZPM3l",
    "outputId": "48f6b999-f2b5-4863-d607-f643a3ccc3de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25890x34383 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 116812 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurized[0]['worked_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXT5bk6tPhF0"
   },
   "outputs": [],
   "source": [
    "train_result = train_models(splits, featurizers=[simple_bag_of_words_featurizer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqajeGiBQbiJ"
   },
   "outputs": [],
   "source": [
    "predictions, true_labels = predict(splits, train_result, split_name='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41813,
     "status": "ok",
     "timestamp": 1607420504773,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "SonjEErwUGQl",
    "outputId": "a0e14be2-51af-4af0-8661-4851338b01f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.860      0.393      0.695        407       7057\n",
      "author                    0.810      0.505      0.723        657       7307\n",
      "capital                   0.681      0.254      0.510        126       6776\n",
      "contains                  0.772      0.599      0.730       4487      11137\n",
      "film_performance          0.787      0.578      0.734        984       7634\n",
      "founders                  0.826      0.414      0.688        469       7119\n",
      "genre                     0.464      0.156      0.333        205       6855\n",
      "has_sibling               0.878      0.254      0.589        625       7275\n",
      "has_spouse                0.901      0.337      0.675        754       7404\n",
      "is_a                      0.690      0.223      0.487        618       7268\n",
      "nationality               0.607      0.168      0.399        386       7036\n",
      "parents                   0.891      0.526      0.782        390       7040\n",
      "place_of_birth            0.633      0.202      0.444        282       6932\n",
      "place_of_death            0.488      0.096      0.268        209       6859\n",
      "profession                0.639      0.201      0.445        308       6958\n",
      "worked_at                 0.712      0.261      0.529        303       6953\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.727      0.323      0.564      11210     117610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5644409661831543"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_predictions(predictions, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtrTth6yrKUD"
   },
   "outputs": [],
   "source": [
    "def find_new_relation_instances(\n",
    "        dataset,\n",
    "        featurizers,\n",
    "        train_split='train',\n",
    "        test_split='dev',\n",
    "        model_factory=(lambda: LogisticRegression(\n",
    "            fit_intercept=True, solver='liblinear', random_state=42)),\n",
    "        k=10,\n",
    "        vectorize=True,\n",
    "        verbose=True):\n",
    "    splits = dataset.build_splits()\n",
    "    # train models\n",
    "    train_result = train_models(\n",
    "        splits,\n",
    "        split_name=train_split,\n",
    "        featurizers=featurizers,\n",
    "        model_factory=model_factory,\n",
    "        vectorize=vectorize,\n",
    "        verbose=True)\n",
    "    test_split = splits[test_split]\n",
    "    neg_o, neg_y = test_split.build_dataset(\n",
    "        include_positive=False,\n",
    "        sampling_rate=1.0)\n",
    "    neg_X, _ = test_split.featurize(\n",
    "        neg_o,\n",
    "        featurizers=featurizers,\n",
    "        vectorizer=train_result['vectorizer'],\n",
    "        vectorize=vectorize)\n",
    "    # Report highest confidence predictions:\n",
    "    for rel, model in train_result['models'].items():\n",
    "        print('Highest probability examples for relation {}:\\n'.format(rel))\n",
    "        probs = model.predict_proba(neg_X[rel])\n",
    "        probs = [prob[1] for prob in probs] # probability for class True\n",
    "        sorted_probs = sorted([(p, idx) for idx, p in enumerate(probs)], reverse=True)\n",
    "        for p, idx in sorted_probs[:k]:\n",
    "            print('{:10.3f} {}'.format(p, neg_o[rel][idx]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39035,
     "status": "ok",
     "timestamp": 1607427893226,
     "user": {
      "displayName": "Mash Po",
      "photoUrl": "",
      "userId": "15661487895586278850"
     },
     "user_tz": -180
    },
    "id": "gKKiVtUCUJ8L",
    "outputId": "8d1c6ea0-44be-4950-d8a6-c0c6c6b0df8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest probability examples for relation adjoins:\n",
      "\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Canada', obj='Vancouver')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Vancouver', obj='Canada')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Sicily', obj='Italy')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Italy', obj='Sicily')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Atlantic_Ocean', obj='Mexico')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Mexico', obj='Atlantic_Ocean')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Lahore', obj='Pakistan')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Pakistan', obj='Lahore')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Europe', obj='Great_Britain')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Great_Britain', obj='Europe')\n",
      "\n",
      "Highest probability examples for relation author:\n",
      "\n",
      "     1.000 KBTriple(rel='author', sbj='Brave_New_World', obj='Aldous_Huxley')\n",
      "     1.000 KBTriple(rel='author', sbj='A_Christmas_Carol', obj='Charles_Dickens')\n",
      "     1.000 KBTriple(rel='author', sbj='Aldous_Huxley', obj='The_Doors_of_Perception')\n",
      "     1.000 KBTriple(rel='author', sbj='The_Doors_of_Perception', obj='Aldous_Huxley')\n",
      "     1.000 KBTriple(rel='author', sbj='Aldous_Huxley', obj='Brave_New_World')\n",
      "     1.000 KBTriple(rel='author', sbj='Charles_Dickens', obj='A_Christmas_Carol')\n",
      "     1.000 KBTriple(rel='author', sbj='Jane_Austen', obj='Pride_and_Prejudice')\n",
      "     1.000 KBTriple(rel='author', sbj='Pride_and_Prejudice', obj='Jane_Austen')\n",
      "     1.000 KBTriple(rel='author', sbj='Divine_Comedy', obj='Dante_Alighieri')\n",
      "     1.000 KBTriple(rel='author', sbj='Dante_Alighieri', obj='Divine_Comedy')\n",
      "\n",
      "Highest probability examples for relation capital:\n",
      "\n",
      "     1.000 KBTriple(rel='capital', sbj='Bangladesh', obj='Dhaka')\n",
      "     1.000 KBTriple(rel='capital', sbj='Uttar_Pradesh', obj='Lucknow')\n",
      "     1.000 KBTriple(rel='capital', sbj='Dhaka', obj='Bangladesh')\n",
      "     1.000 KBTriple(rel='capital', sbj='Lucknow', obj='Uttar_Pradesh')\n",
      "     1.000 KBTriple(rel='capital', sbj='Chengdu', obj='Sichuan')\n",
      "     1.000 KBTriple(rel='capital', sbj='Sichuan', obj='Chengdu')\n",
      "     1.000 KBTriple(rel='capital', sbj='India', obj='Delhi')\n",
      "     1.000 KBTriple(rel='capital', sbj='Delhi', obj='India')\n",
      "     1.000 KBTriple(rel='capital', sbj='West_Java', obj='Bandung')\n",
      "     1.000 KBTriple(rel='capital', sbj='Bandung', obj='West_Java')\n",
      "\n",
      "Highest probability examples for relation contains:\n",
      "\n",
      "     1.000 KBTriple(rel='contains', sbj='Bangladesh', obj='Dhaka')\n",
      "     1.000 KBTriple(rel='contains', sbj='Sydney', obj='New_South_Wales')\n",
      "     1.000 KBTriple(rel='contains', sbj='New_South_Wales', obj='Sydney')\n",
      "     1.000 KBTriple(rel='contains', sbj='Lahore', obj='Pakistan')\n",
      "     1.000 KBTriple(rel='contains', sbj='Canada', obj='Vancouver')\n",
      "     1.000 KBTriple(rel='contains', sbj='Australia', obj='Melbourne')\n",
      "     1.000 KBTriple(rel='contains', sbj='Naples', obj='Campania')\n",
      "     1.000 KBTriple(rel='contains', sbj='Sydney', obj='Australia')\n",
      "     1.000 KBTriple(rel='contains', sbj='United_Arab_Emirates', obj='Dubai')\n",
      "     1.000 KBTriple(rel='contains', sbj='Campania', obj='Naples')\n",
      "\n",
      "Highest probability examples for relation film_performance:\n",
      "\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Mohabbatein', obj='Amitabh_Bachchan')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Amitabh_Bachchan', obj='Mohabbatein')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='A_Christmas_Carol', obj='Charles_Dickens')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Charles_Dickens', obj='A_Christmas_Carol')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='De-Lovely', obj='Kevin_Kline')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Kevin_Kline', obj='De-Lovely')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Akshay_Kumar', obj='Sonakshi_Sinha')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Sonakshi_Sinha', obj='Akshay_Kumar')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Hrithik_Roshan', obj='Kaho_Naa..._Pyaar_Hai')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Kaho_Naa..._Pyaar_Hai', obj='Hrithik_Roshan')\n",
      "\n",
      "Highest probability examples for relation founders:\n",
      "\n",
      "     1.000 KBTriple(rel='founders', sbj='Louis_Chevrolet', obj='William_C._Durant')\n",
      "     1.000 KBTriple(rel='founders', sbj='William_C._Durant', obj='Louis_Chevrolet')\n",
      "     1.000 KBTriple(rel='founders', sbj='Iliad', obj='Homer')\n",
      "     1.000 KBTriple(rel='founders', sbj='Homer', obj='Iliad')\n",
      "     1.000 KBTriple(rel='founders', sbj='Stan_Lee', obj='Marvel_Comics')\n",
      "     1.000 KBTriple(rel='founders', sbj='Marvel_Comics', obj='Stan_Lee')\n",
      "     1.000 KBTriple(rel='founders', sbj='SpaceX', obj='Elon_Musk')\n",
      "     1.000 KBTriple(rel='founders', sbj='Elon_Musk', obj='SpaceX')\n",
      "     1.000 KBTriple(rel='founders', sbj='Genghis_Khan', obj='Mongol_Empire')\n",
      "     1.000 KBTriple(rel='founders', sbj='Mongol_Empire', obj='Genghis_Khan')\n",
      "\n",
      "Highest probability examples for relation genre:\n",
      "\n",
      "     0.994 KBTriple(rel='genre', sbj='Mark_Twain_Tonight', obj='Hal_Holbrook')\n",
      "     0.994 KBTriple(rel='genre', sbj='Hal_Holbrook', obj='Mark_Twain_Tonight')\n",
      "     0.987 KBTriple(rel='genre', sbj='Pink_Floyd', obj='The_Dark_Side_of_the_Moon')\n",
      "     0.987 KBTriple(rel='genre', sbj='The_Dark_Side_of_the_Moon', obj='Pink_Floyd')\n",
      "     0.987 KBTriple(rel='genre', sbj='Andrew_Garfield', obj='Sam_Raimi')\n",
      "     0.987 KBTriple(rel='genre', sbj='Sam_Raimi', obj='Andrew_Garfield')\n",
      "     0.976 KBTriple(rel='genre', sbj='De-Lovely', obj='Kevin_Kline')\n",
      "     0.976 KBTriple(rel='genre', sbj='Kevin_Kline', obj='De-Lovely')\n",
      "     0.947 KBTriple(rel='genre', sbj='JYP_Entertainment', obj='South_Korea')\n",
      "     0.947 KBTriple(rel='genre', sbj='South_Korea', obj='JYP_Entertainment')\n",
      "\n",
      "Highest probability examples for relation has_sibling:\n",
      "\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='April_Margera', obj='Jess_Margera')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Jess_Margera', obj='April_Margera')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Gutzon_Borglum', obj='Lincoln_Borglum')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Lincoln_Borglum', obj='Gutzon_Borglum')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Kate_McGarrigle', obj='Rufus_Wainwright')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Rufus_Wainwright', obj='Kate_McGarrigle')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Ronald_Goldman', obj='Nicole_Brown_Simpson')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Nicole_Brown_Simpson', obj='Ronald_Goldman')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Dionne_Warwick', obj='Aretha_Franklin')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Aretha_Franklin', obj='Dionne_Warwick')\n",
      "\n",
      "Highest probability examples for relation has_spouse:\n",
      "\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Tutankhamun', obj='Akhenaten')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Akhenaten', obj='Tutankhamun')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='United_Artists', obj='Douglas_Fairbanks')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Douglas_Fairbanks', obj='United_Artists')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Louis_Chevrolet', obj='William_C._Durant')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='William_C._Durant', obj='Louis_Chevrolet')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Ronald_Goldman', obj='Nicole_Brown_Simpson')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Nicole_Brown_Simpson', obj='Ronald_Goldman')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='England', obj='Charles_II_of_England')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Charles_II_of_England', obj='England')\n",
      "\n",
      "Highest probability examples for relation is_a:\n",
      "\n",
      "     1.000 KBTriple(rel='is_a', sbj='Felidae', obj='Panthera')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Panthera', obj='Felidae')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Canada', obj='Vancouver')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Vancouver', obj='Canada')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Phasianidae', obj='Bird')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Bird', obj='Phasianidae')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Bird', obj='Accipitridae')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Accipitridae', obj='Bird')\n",
      "     1.000 KBTriple(rel='is_a', sbj='South_Korea', obj='Automobile')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Automobile', obj='South_Korea')\n",
      "\n",
      "Highest probability examples for relation nationality:\n",
      "\n",
      "     1.000 KBTriple(rel='nationality', sbj='Titus', obj='Roman_Empire')\n",
      "     1.000 KBTriple(rel='nationality', sbj='Roman_Empire', obj='Titus')\n",
      "     1.000 KBTriple(rel='nationality', sbj='Norodom_Sihamoni', obj='Cambodia')\n",
      "     1.000 KBTriple(rel='nationality', sbj='Cambodia', obj='Norodom_Sihamoni')\n",
      "     1.000 KBTriple(rel='nationality', sbj='Genghis_Khan', obj='Mongol_Empire')\n",
      "     1.000 KBTriple(rel='nationality', sbj='Mongol_Empire', obj='Genghis_Khan')\n",
      "     1.000 KBTriple(rel='nationality', sbj='Nepal', obj='Bagmati_Zone')\n",
      "     1.000 KBTriple(rel='nationality', sbj='Bagmati_Zone', obj='Nepal')\n",
      "     1.000 KBTriple(rel='nationality', sbj='April_Margera', obj='Jess_Margera')\n",
      "     1.000 KBTriple(rel='nationality', sbj='Jess_Margera', obj='April_Margera')\n",
      "\n",
      "Highest probability examples for relation parents:\n",
      "\n",
      "     1.000 KBTriple(rel='parents', sbj='Gutzon_Borglum', obj='Lincoln_Borglum')\n",
      "     1.000 KBTriple(rel='parents', sbj='Philip_II_of_Macedon', obj='Alexander_the_Great')\n",
      "     1.000 KBTriple(rel='parents', sbj='Alexander_the_Great', obj='Philip_II_of_Macedon')\n",
      "     1.000 KBTriple(rel='parents', sbj='Lincoln_Borglum', obj='Gutzon_Borglum')\n",
      "     1.000 KBTriple(rel='parents', sbj='Anne_Boleyn', obj='Thomas_Boleyn,_1st_Earl_of_Wiltshire')\n",
      "     1.000 KBTriple(rel='parents', sbj='Thomas_Boleyn,_1st_Earl_of_Wiltshire', obj='Anne_Boleyn')\n",
      "     1.000 KBTriple(rel='parents', sbj='Gwyneth_Paltrow', obj='Bruce_Paltrow')\n",
      "     1.000 KBTriple(rel='parents', sbj='Bruce_Paltrow', obj='Gwyneth_Paltrow')\n",
      "     1.000 KBTriple(rel='parents', sbj='April_Margera', obj='Jess_Margera')\n",
      "     1.000 KBTriple(rel='parents', sbj='Jess_Margera', obj='April_Margera')\n",
      "\n",
      "Highest probability examples for relation place_of_birth:\n",
      "\n",
      "     1.000 KBTriple(rel='place_of_birth', sbj='Nepal', obj='Bagmati_Zone')\n",
      "     1.000 KBTriple(rel='place_of_birth', sbj='Bagmati_Zone', obj='Nepal')\n",
      "     0.997 KBTriple(rel='place_of_birth', sbj='Lumbini', obj='Gautama_Buddha')\n",
      "     0.997 KBTriple(rel='place_of_birth', sbj='Gautama_Buddha', obj='Lumbini')\n",
      "     0.995 KBTriple(rel='place_of_birth', sbj='Julian_Castro', obj='San_Antonio')\n",
      "     0.995 KBTriple(rel='place_of_birth', sbj='San_Antonio', obj='Julian_Castro')\n",
      "     0.993 KBTriple(rel='place_of_birth', sbj='Genghis_Khan', obj='Mongol_Empire')\n",
      "     0.993 KBTriple(rel='place_of_birth', sbj='Mongol_Empire', obj='Genghis_Khan')\n",
      "     0.992 KBTriple(rel='place_of_birth', sbj='Titus', obj='Roman_Empire')\n",
      "     0.992 KBTriple(rel='place_of_birth', sbj='Roman_Empire', obj='Titus')\n",
      "\n",
      "Highest probability examples for relation place_of_death:\n",
      "\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Titus', obj='Roman_Empire')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Roman_Empire', obj='Titus')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Philip_II_of_Macedon', obj='Alexander_the_Great')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Alexander_the_Great', obj='Philip_II_of_Macedon')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Genghis_Khan', obj='Mongol_Empire')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Mongol_Empire', obj='Genghis_Khan')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='England', obj='Elizabeth_I_of_England')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Elizabeth_I_of_England', obj='England')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Roman_Empire', obj='Trajan')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Trajan', obj='Roman_Empire')\n",
      "\n",
      "Highest probability examples for relation profession:\n",
      "\n",
      "     1.000 KBTriple(rel='profession', sbj='Canada', obj='Vancouver')\n",
      "     1.000 KBTriple(rel='profession', sbj='Vancouver', obj='Canada')\n",
      "     1.000 KBTriple(rel='profession', sbj='Actor', obj='Screenwriter')\n",
      "     1.000 KBTriple(rel='profession', sbj='Screenwriter', obj='Actor')\n",
      "     0.999 KBTriple(rel='profession', sbj='Little_Women', obj='Louisa_May_Alcott')\n",
      "     0.999 KBTriple(rel='profession', sbj='Louisa_May_Alcott', obj='Little_Women')\n",
      "     0.999 KBTriple(rel='profession', sbj='Eyeless_in_Gaza', obj='Aldous_Huxley')\n",
      "     0.999 KBTriple(rel='profession', sbj='Aldous_Huxley', obj='Eyeless_in_Gaza')\n",
      "     0.998 KBTriple(rel='profession', sbj='Multi-instrumentalist', obj='Musician')\n",
      "     0.998 KBTriple(rel='profession', sbj='Musician', obj='Multi-instrumentalist')\n",
      "\n",
      "Highest probability examples for relation worked_at:\n",
      "\n",
      "     1.000 KBTriple(rel='worked_at', sbj='Louis_Chevrolet', obj='William_C._Durant')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='William_C._Durant', obj='Louis_Chevrolet')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='Iliad', obj='Homer')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='Homer', obj='Iliad')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='SpaceX', obj='Elon_Musk')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='Elon_Musk', obj='SpaceX')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='Genghis_Khan', obj='Mongol_Empire')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='Mongol_Empire', obj='Genghis_Khan')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='Marvel_Comics', obj='Comic_book')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='Comic_book', obj='Marvel_Comics')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_new_relation_instances(dataset,featurizers=[simple_bag_of_words_featurizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhyfxOlHsREO"
   },
   "source": [
    "### Задание (подумать и попробовать реализовать) \n",
    "Итак, мешок слов, несмотря на свою простоту, показывает себя довольно хорошо. Как более хитро можно извлечь признаки из предложений, иллюстрирующих пары сущностей?  В качестве задания вам предлагается улучшить бейслайн, например придумав, как использовать векторизацию контекстов для предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cF3hIwT4xFYV"
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS for RE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (36-64)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
